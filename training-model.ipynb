{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled15.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "CH_QSwCU2Eo3"
   },
   "source": [
    "# python packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout, SeparableConv2D, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import datasets, metrics"
   ],
   "execution_count": 151,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4Sbdupw2HSw",
    "outputId": "852d8c93-9099-45e7-c7b3-a9fd3e28f981",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 152,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9taplZBg2KxI"
   },
   "source": [
    "IMAGE_SIZE=[150,150]"
   ],
   "execution_count": 153,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXImg_Sa2OcU",
    "outputId": "ca1edf99-440f-448e-e247-ebc118025ccb"
   },
   "source": [
    "datagen = ImageDataGenerator(rescale = 1./255, \n",
    "                             shear_range= 0.2, \n",
    "                             horizontal_flip= True, \n",
    "                             validation_split= 0.2,\n",
    "                             dtype=tensorflow.float32)\n",
    "\n",
    "\n",
    "training_set = datagen.flow_from_directory('/content/drive/MyDrive/datasets', \n",
    "                                           \n",
    "                                           target_size=IMAGE_SIZE, \n",
    "                                           batch_size=64, \n",
    "                                           class_mode='binary',\n",
    "                                           subset='training',\n",
    "                                           shuffle=True, \n",
    "                                           seed=123, )\n",
    "\n",
    "test_set = datagen.flow_from_directory('/content/drive/MyDrive/datasets',\n",
    "                                       target_size=IMAGE_SIZE,\n",
    "                                       batch_size=64, \n",
    "                                       class_mode='binary', \n",
    "                                       subset='validation', \n",
    "                                       shuffle=True, \n",
    "                                       seed=123, )"
   ],
   "execution_count": 154,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Found 1981 images belonging to 2 classes.\n",
      "Found 494 images belonging to 2 classes.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7VQKXlfs4VM-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Flatten, Dense, Conv2D, AveragePooling2D, BatchNormalization, SpatialDropout2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from pathlib import Path"
   ],
   "execution_count": 155,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JEBbHIiV4WD1"
   },
   "source": [
    "nb_classes = 2"
   ],
   "execution_count": 156,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gJIkVZUN4d_v"
   },
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', input_shape=IMAGE_SIZE + [3],padding='same',))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same',))\n",
    "tensorflow.keras.layers.MaxPool2D(),\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same',))\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same',))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same',))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same',))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ],
   "execution_count": 157,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zzBNfwwaZrOW"
   },
   "source": [
    "sgd = tensorflow.keras.optimizers.SGD(learning_rate=0.01)"
   ],
   "execution_count": 158,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PMZtb-0xhNdc"
   },
   "source": [],
   "execution_count": 158,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "00KeH0UxgLnB"
   },
   "source": [
    "initial_learning_rate = 0.01\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.1\n",
    "    return initial_learning_rate * math.exp(-k*epoch)"
   ],
   "execution_count": 159,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vTRwmUYggoZg"
   },
   "source": [],
   "execution_count": 159,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CjdDtdPT4eKz"
   },
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer= sgd, metrics = ['accuracy'])"
   ],
   "execution_count": 160,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_yLTM3gCG5R",
    "outputId": "e3cfcac6-c23a-466f-a9e8-9bc59bfb69fd"
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": 161,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_92 (Conv2D)           (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_93 (Conv2D)           (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 150, 150, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 150, 150, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 75, 75, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 350464)            0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 512)               179438080 \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 180,659,009\n",
      "Trainable params: 180,658,241\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "gNqUYbUh4eVX",
    "outputId": "a37bd8b9-7e30-4c8a-e982-e507d140f43d"
   },
   "source": [
    "'''model= Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size= (3,3), activation='tanh', input_shape=IMAGE_SIZE + [3],padding='same',strides=(1,1)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2), padding= 'valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size= (3,3), activation= 'relu', input_shape= IMAGE_SIZE + [3], padding='same',\n",
    "                strides= (1,1)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['accuracy'])'''"
   ],
   "execution_count": 162,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"model= Sequential()\\n\\nmodel.add(Conv2D(64, kernel_size= (3,3), activation='tanh', input_shape=IMAGE_SIZE + [3],padding='same',strides=(1,1)))\\nmodel.add(AveragePooling2D(pool_size=(2,2), strides=(2,2), padding= 'valid'))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.5))\\n\\nmodel.add(Conv2D(64, kernel_size= (3,3), activation= 'relu', input_shape= IMAGE_SIZE + [3], padding='same',\\n                strides= (1,1)))\\nmodel.add(AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\\nmodel.add(BatchNormalization())\\n\\nmodel.add(Dropout(0.3))\\n\\nmodel.add(Flatten())\\nmodel.add(Dropout(0.3))\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dropout(0.3))\\nmodel.add(Dense(64, activation='relu'))\\nmodel.add(Dropout(0.3))\\nmodel.add(Dense(32, activation='relu'))\\nmodel.add(Dropout(0.3))\\nmodel.add(Dense(1, activation = 'sigmoid'))\\n\\nmodel.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['accuracy'])\""
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 162
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MRqMOxYliZ-r"
   },
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.1\n",
    "    decay_step = 90\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "callbacks = [\n",
    "    tensorflow.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "]"
   ],
   "execution_count": 163,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3G2ioRW14emh",
    "outputId": "e42936d5-435e-4f86-af8b-46498b335747"
   },
   "source": [
    "history1 = model.fit(training_set,epochs=34,steps_per_epoch=10,\n",
    "                    validation_steps=7,validation_data=test_set, callbacks=callbacks,shuffle=True)"
   ],
   "execution_count": 165,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/34\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 29s 1s/step - loss: 6.8901 - accuracy: 0.5197 - val_loss: 0.7512 - val_accuracy: 0.5000\n",
      "Epoch 2/34\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 2.4573 - accuracy: 0.4926 - val_loss: 0.6779 - val_accuracy: 0.5379\n",
      "Epoch 3/34\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 27s 3s/step - loss: 1.2679 - accuracy: 0.5464 - val_loss: 0.6868 - val_accuracy: 0.4978\n",
      "Epoch 4/34\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.8894 - accuracy: 0.5338 - val_loss: 0.6724 - val_accuracy: 0.5179\n",
      "Epoch 5/34\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.8807 - accuracy: 0.4948 - val_loss: 0.7001 - val_accuracy: 0.4933\n",
      "Epoch 6/34\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.7186 - accuracy: 0.5567 - val_loss: 0.6726 - val_accuracy: 0.5335\n",
      "Epoch 7/34\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.7189 - accuracy: 0.5519 - val_loss: 0.6583 - val_accuracy: 0.5379\n",
      "Epoch 8/34\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6703 - accuracy: 0.5633 - val_loss: 0.6547 - val_accuracy: 0.5938\n",
      "Epoch 9/34\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6455 - accuracy: 0.6255 - val_loss: 0.6577 - val_accuracy: 0.6049\n",
      "Epoch 10/34\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6916 - accuracy: 0.5730 - val_loss: 0.6578 - val_accuracy: 0.6339\n",
      "Epoch 11/34\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6600 - accuracy: 0.5785 - val_loss: 0.6528 - val_accuracy: 0.6451\n",
      "Epoch 12/34\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6962 - accuracy: 0.5772 - val_loss: 0.6566 - val_accuracy: 0.6183\n",
      "Epoch 13/34\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6782 - accuracy: 0.5481 - val_loss: 0.6533 - val_accuracy: 0.6518\n",
      "Epoch 14/34\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6491 - accuracy: 0.5865 - val_loss: 0.6705 - val_accuracy: 0.5759\n",
      "Epoch 15/34\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6152 - accuracy: 0.6248 - val_loss: 0.6682 - val_accuracy: 0.5603\n",
      "Epoch 16/34\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6520 - accuracy: 0.6351 - val_loss: 0.6527 - val_accuracy: 0.6295\n",
      "Epoch 17/34\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6330 - accuracy: 0.6341 - val_loss: 0.6407 - val_accuracy: 0.7277\n",
      "Epoch 18/34\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6595 - accuracy: 0.6230 - val_loss: 0.6423 - val_accuracy: 0.7232\n",
      "Epoch 19/34\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6326 - accuracy: 0.6499 - val_loss: 0.6453 - val_accuracy: 0.6964\n",
      "Epoch 20/34\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6279 - accuracy: 0.6883 - val_loss: 0.6330 - val_accuracy: 0.6518\n",
      "Epoch 21/34\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6099 - accuracy: 0.6431 - val_loss: 0.6445 - val_accuracy: 0.5223\n",
      "Epoch 22/34\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6083 - accuracy: 0.6854 - val_loss: 0.6357 - val_accuracy: 0.5513\n",
      "Epoch 23/34\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5599 - accuracy: 0.6747 - val_loss: 0.6407 - val_accuracy: 0.5357\n",
      "Epoch 24/34\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5925 - accuracy: 0.6949 - val_loss: 0.6488 - val_accuracy: 0.5223\n",
      "Epoch 25/34\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6018 - accuracy: 0.6730 - val_loss: 0.6737 - val_accuracy: 0.5156\n",
      "Epoch 26/34\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5721 - accuracy: 0.6877 - val_loss: 0.6776 - val_accuracy: 0.5134\n",
      "Epoch 27/34\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5469 - accuracy: 0.7137 - val_loss: 0.6741 - val_accuracy: 0.5022\n",
      "Epoch 28/34\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5879 - accuracy: 0.7243 - val_loss: 0.7069 - val_accuracy: 0.5201\n",
      "Epoch 29/34\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5627 - accuracy: 0.6963 - val_loss: 0.7023 - val_accuracy: 0.5089\n",
      "Epoch 30/34\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5663 - accuracy: 0.7086 - val_loss: 0.7186 - val_accuracy: 0.5022\n",
      "Epoch 31/34\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5503 - accuracy: 0.7164 - val_loss: 0.7363 - val_accuracy: 0.4933\n",
      "Epoch 32/34\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5513 - accuracy: 0.7098 - val_loss: 0.7437 - val_accuracy: 0.5089\n",
      "Epoch 33/34\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.5247 - accuracy: 0.7526 - val_loss: 0.8089 - val_accuracy: 0.4866\n",
      "Epoch 34/34\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5732 - accuracy: 0.7398"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-165-f63346c2f134>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m history1 = model.fit(training_set,epochs=34,steps_per_epoch=10,\n\u001B[0;32m----> 2\u001B[0;31m                     validation_steps=7,validation_data=test_set, callbacks=callbacks,shuffle=True)\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1103\u001B[0m               \u001B[0mlogs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtmp_logs\u001B[0m  \u001B[0;31m# No error, now safe to assign to logs.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m               \u001B[0mend_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstep\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep_increment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1105\u001B[0;31m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mend_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1106\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_training\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1107\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36mon_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m    452\u001B[0m     \"\"\"\n\u001B[1;32m    453\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_call_train_batch_hooks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 454\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_batch_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mModeKeys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTRAIN\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'end'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    455\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mon_test_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36m_call_batch_hook\u001B[0;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[1;32m    294\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_batch_begin_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    295\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'end'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 296\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_batch_end_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    297\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    298\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Unrecognized hook: {}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36m_call_batch_end_hook\u001B[0;34m(self, mode, batch, logs)\u001B[0m\n\u001B[1;32m    314\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_batch_times\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 316\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_batch_hook_helper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhook_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    317\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    318\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_batch_times\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_batches_for_timing_check\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36m_call_batch_hook_helper\u001B[0;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[1;32m    354\u001B[0m       \u001B[0mhook\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcallback\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    355\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcallback\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'_supports_tf_logs'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 356\u001B[0;31m         \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    357\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    358\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnumpy_logs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Only convert once.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36mon_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1018\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1019\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mon_train_batch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1020\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_batch_update_progbar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1021\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1022\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mon_test_batch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36m_batch_update_progbar\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1082\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1083\u001B[0m       \u001B[0;31m# Only block async when verbose = 1.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1084\u001B[0;31m       \u001B[0mlogs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy_or_python_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1085\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprogbar\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseen\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfinalize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1086\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001B[0m in \u001B[0;36mto_numpy_or_python_type\u001B[0;34m(tensors)\u001B[0m\n\u001B[1;32m    512\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mt\u001B[0m  \u001B[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 514\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_single_numpy_or_python_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    515\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001B[0m in \u001B[0;36mmap_structure\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    657\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    658\u001B[0m   return pack_sequence_as(\n\u001B[0;32m--> 659\u001B[0;31m       \u001B[0mstructure\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    660\u001B[0m       expand_composites=expand_composites)\n\u001B[1;32m    661\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    657\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    658\u001B[0m   return pack_sequence_as(\n\u001B[0;32m--> 659\u001B[0;31m       \u001B[0mstructure\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    660\u001B[0m       expand_composites=expand_composites)\n\u001B[1;32m    661\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001B[0m in \u001B[0;36m_to_single_numpy_or_python_type\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    508\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_to_single_numpy_or_python_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    509\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 510\u001B[0;31m       \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    511\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    512\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mt\u001B[0m  \u001B[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mnumpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1069\u001B[0m     \"\"\"\n\u001B[1;32m   1070\u001B[0m     \u001B[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1071\u001B[0;31m     \u001B[0mmaybe_arr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1072\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmaybe_arr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmaybe_arr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mmaybe_arr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1073\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36m_numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1035\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1036\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1037\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_numpy_internal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1038\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1039\u001B[0m       \u001B[0msix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4B_I33yB4esb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure.json\")\n",
    "f.write_text(model_structure)\n",
    "\n",
    "model.save_weights('model_weights.h5')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8x_8FbCaOUW0"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "or86U3m24ex5"
   },
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "\n",
    "class_label = [\"COVID-19 CASE\", \"NON COVID-19 CASE\"]\n",
    "\n",
    "#test_image= image.load_img('/content/drive/MyDrive/FinalYearProject/datasets/CT_NonCOVID/1078.png', target_size = (64,64))\n",
    "#Wrong-test_image= image.load_img('/content/drive/MyDrive/FinalYearProject/datasets/CT_NonCOVID/1859.png', target_size = (64,64))\n",
    "#test_image= image.load_img('/content/drive/MyDrive/FinalYearProject/datasets/CT_NonCOVID/887.png', target_size = (64,64))\n",
    "#test_image= image.load_img('/content/drive/MyDrive/FinalYearProject/datasets/CT_NonCOVID/3.jpg', target_size = (64,64))\n",
    "#test_image= image.load_img('/content/drive/MyDrive/FinalYearProject/datasets/CT_COVID/2020.03.18.20038125-p16-56-2.png', target_size = (64,64))\n",
    "\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()\n",
    "\n",
    "model = model_from_json(model_structure)\n",
    "\n",
    "model.load_weights(\"model_weights.h5\")\n",
    "\n",
    "uploaded_img = image.load_img('/content/drive/MyDrive/heart image scan-300x225.jpg')\n",
    "img= image.load_img('/content/drive/MyDrive/heart image scan-300x225.jpg', target_size = IMAGE_SIZE+ [3])\n",
    "\n",
    "test_image = image.img_to_array(img)/255\n",
    "\n",
    "listOfImages = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = model.predict(listOfImages)\n",
    "\n",
    "single_result = result[0]\n",
    "\n",
    "mostLikelyClass = int(np.argmax(single_result))\n",
    "\n",
    "class_likelihood = single_result[mostLikelyClass]\n",
    "\n",
    "class_label = class_label[mostLikelyClass]\n",
    "print(\"image is a {} - likelihood: {:2f}\".format(class_label, class_likelihood))\n",
    "\n",
    "\n",
    "#classes = training_set.class_indices\n",
    "\n",
    "#print(training_set.class_indices)\n",
    "if result[0][0] >= 0.7 :\n",
    "  prediction_Model = 'PREDICTION: Non Covid (Healthy)'\n",
    "elif result[0][0] <= 0.05:\n",
    "  prediction_Model = 'PREDICTION: Non Covid (Healthy) or scan is irelevant'\n",
    "else:\n",
    "  prediction_Model = 'PREDICTION: Covid Case'\n",
    "print('\\n\\n')\n",
    "print(prediction_Model)\n",
    "plt.title('UPLOADED IMAGE')\n",
    "plt.imshow(uploaded_img)\n",
    "plt.xlabel('Uploaded CT scan image')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tGVAxBXU5LPa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(history1.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('model3 accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model3 loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}